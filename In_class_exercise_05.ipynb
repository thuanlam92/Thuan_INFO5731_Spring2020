{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "In_class_exercise_05.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNhAzLzgNUfxtSzNGzyIUwO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thuanlam92/Thuan_INFO5731_Spring2020/blob/master/In_class_exercise_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCkMQPB9bhmg",
        "colab_type": "text"
      },
      "source": [
        "## **1. Rule-based information extraction**\n",
        "\n",
        "Use any keywords related to data science, natural language processing, machine learning to search from google scholar, get the **titles** of 100 articles (either by web scraping or manually) about this topic, define a set of patterns to extract the research questions/problems, methods/algorithms/models, datasets, applications, or any other important information about this topic. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0v59IHou_G8k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import requests, sys, webbrowser, bs4\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNceAHYiccSq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # get the titles\n",
        "\n",
        "# user_input = input(\"enter something to search:\")\n",
        "# print(\"googling.....\")\n",
        "\n",
        "\n",
        "# google_search = requests.get(\"https://www.google.com/search?q=\" + user_input)\n",
        "\n",
        "# soup = BeautifulSoup(google_search.text, 'html.parser')\n",
        "# print(soup.prettify())\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtZJjhYLcwF3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "!pip install google\n",
        "import bs4\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "try: \n",
        "\tfrom googlesearch import search \n",
        "except ImportError: \n",
        "\tprint(\"No module named 'google' found\") \n",
        "\n",
        "# to search \n",
        "query = \"Big Data\"\n",
        "\n",
        "for j in search(query, tld=\"co.in\", num=100): \n",
        "\tprint(j) \n",
        " \n",
        "\n",
        "source_code = requests.get(j)\n",
        "plain_text = source_code.content\n",
        "soup = BeautifulSoup(plain_text, \"lxml\")\n",
        "# links = soup.findAll('h1', {'class': ''})\n",
        "# print len(links)\n",
        "# for link in links:\n",
        "#     title = link.get('title')\n",
        "#     print title\n",
        "  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYFAujVYss2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "#create json\n",
        "data = {\n",
        "    \"authors\": [],\n",
        "    \"coAuthors\": [],\n",
        "    \"pageSize\": 10,\n",
        "    \"queryString\": \"big data\", #keyword to search: big data. can be any keyword\n",
        "    \"sort\": \"relevance\",\n",
        "    \"venues\": [],\n",
        "}\n",
        "\n",
        "\n",
        "#get titles\n",
        "\n",
        "titles=[]\n",
        "\n",
        "#10 pages = 100 articles\n",
        "for item in range(1,11):  \n",
        "    data['page'] = item\n",
        "    req = requests.post('https://www.semanticscholar.org/api/1/search', json=data).json()\n",
        "    y = req['results']\n",
        "\n",
        "    for x in y:\n",
        "        titles.append(x['title']['text'])\n",
        "        \n",
        "\n",
        "print(titles)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn1V1YSVbk-7",
        "colab_type": "text"
      },
      "source": [
        "## **2. Domain-specific information extraction**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "For the legal case used in the data cleaning exercise: [01-05-1 Adams v Tanner.txt](https://raw.githubusercontent.com/unt-iialab/INFO5731_Spring2020/master/In_class_exercise/01-05-1%20%20Adams%20v%20Tanner.txt), use [legalNLP](https://lexpredict-lexnlp.readthedocs.io/en/latest/modules/extract/extract.html#nlp-based-extraction-methods) to extract the following inforation from the text (if the information is not exist, just print None):\n",
        "\n",
        "(1) acts, e.g., “section 1 of the Advancing Hope Act, 1986”\n",
        "\n",
        "(2) amounts, e.g., “ten pounds” or “5.8 megawatts”\n",
        "\n",
        "(3) citations, e.g., “10 U.S. 100” or “1998 S. Ct. 1”\n",
        "\n",
        "(4) companies, e.g., “Lexpredict LLC”\n",
        "\n",
        "(5) conditions, e.g., “subject to …” or “unless and until …”\n",
        "\n",
        "(6) constraints, e.g., “no more than”\n",
        "\n",
        "(7) copyright, e.g., “(C) Copyright 2000 Acme”\n",
        "\n",
        "(8) courts, e.g., “Supreme Court of New York”\n",
        "\n",
        "(9) CUSIP, e.g., “392690QT3”\n",
        "\n",
        "(10) dates, e.g., “June 1, 2017” or “2018-01-01”\n",
        "\n",
        "(11) definitions, e.g., “Term shall mean …”\n",
        "\n",
        "(12) distances, e.g., “fifteen miles”\n",
        "\n",
        "(13) durations, e.g., “ten years” or “thirty days”\n",
        "\n",
        "(14) geographic and geopolitical entities, e.g., “New York” or “Norway”\n",
        "\n",
        "(15) money and currency usages, e.g., “$5” or “10 Euro”\n",
        "\n",
        "(16) percents and rates, e.g., “10%” or “50 bps”\n",
        "\n",
        "(17) PII, e.g., “212-212-2121” or “999-999-9999”\n",
        "\n",
        "(18) ratios, e.g.,” 3:1” or “four to three”\n",
        "\n",
        "(19) regulations, e.g., “32 CFR 170”\n",
        "\n",
        "(20) trademarks, e.g., “MyApp (TM)”\n",
        "\n",
        "(21) URLs, e.g., “http://acme.com/”\n",
        "\n",
        "(22) addresses, e.g., “1999 Mount Read Blvd, Rochester, NY, USA, 14615”\n",
        "\n",
        "(23) persons, e.g., “John Doe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B41rrSSMkW5K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install lexnlp\n",
        "!pip install nltk\n",
        "!pip install pysbd\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_PuVEGQ7TDec",
        "colab": {}
      },
      "source": [
        "import pysbd\n",
        "import spacy\n",
        "from pysbd.utils import PySBDFactory\n",
        "import lexnlp.extract.en.amounts\n",
        "\n",
        "\n",
        "\n",
        "f = open('AdamsTanner.txt','r', encoding='utf8', errors = \"ignore\")\n",
        "lines = f.readlines()\n",
        "\n",
        "\n",
        "#break text into sentences\n",
        "\n",
        "all_sent = []\n",
        "for line in lines:\n",
        "  \n",
        "    # print(line)\n",
        "    nlp = spacy.blank('en')\n",
        "    nlp.add_pipe(PySBDFactory(nlp))\n",
        "    doc = nlp(line)\n",
        "    # print(doc)\n",
        "    # print(list(doc.sents))\n",
        "    for item in list(doc.sents):\n",
        "      all_sent.append(item)\n",
        "# print(all_sent[1])\n",
        "\n",
        "#run through each sentence\n",
        "\n",
        "all_amount = []\n",
        "\n",
        "for sent in all_sent:\n",
        "  r = lexnlp.extract.en.amounts.get_amounts(sent)\n",
        "  all_amount.append(r)\n",
        "print(all_amount[1])\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}